{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbd39f8",
   "metadata": {},
   "source": [
    " Evaluate logistic and multivariate logistic models using ROC-AUC, precision, recall, and\n",
    " confusionmatrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87886d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "#------------------------\n",
    "# Binary Logistic Regression\n",
    "#------------------------\n",
    "# Filter for Setosa and Versicolor (classes 0 and 1)\n",
    "binary_filter = y < 2\n",
    "X_bin = X[binary_filter]\n",
    "y_bin = y[binary_filter]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e6883",
   "metadata": {},
   "source": [
    "### Explanation for data loading and preprocessing (Cell below imports and prepares Iris dataset)\n",
    "\n",
    "- import numpy as np: imports NumPy for numerical operations (arrays, math)\n",
    "- import pandas as pd: imports pandas for DataFrame operations (not strictly used later but common practice)\n",
    "- from sklearn.datasets import load_iris: imports a function to load the Iris dataset used for examples\n",
    "- from sklearn.linear_model import LogisticRegression: imports the classifier used for both binary and multiclass logistic regression\n",
    "- from sklearn.metrics import (...): imports metric functions for evaluation such as confusion_matrix, roc_auc_score, roc_curve and auc\n",
    "  - classification_report: gives precision, recall, f1-score per class\n",
    "  - confusion_matrix: tabulates true vs predicted class counts\n",
    "  - roc_auc_score: numeric AUC score for ROC curve (works for binary and binarized multiclass)\n",
    "  - roc_curve: computes false/true positive rates for different thresholds\n",
    "  - auc: computes area under curve from fpr and tpr arrays\n",
    "- from sklearn.preprocessing import label_binarize: used to convert multiclass labels into binary indicator matrix required for multiclass ROC-AUC\n",
    "- import matplotlib.pyplot as plt: plotting library used for ROC curves\n",
    "- import seaborn as sns: plotting themeing (not used but often helpful)\n",
    "\n",
    "- iris = load_iris(): loads the dataset as a Bunch object with .data and .target\n",
    "- X = iris.data: feature matrix (shape: samples x features)\n",
    "- y = iris.target: integer labels (0,1,2)\n",
    "- target_names = iris.target_names: string names for classes\n",
    "\n",
    "- binary_filter = y < 2: selects only class 0 and 1 for a binary example\n",
    "- X_bin = X[binary_filter], y_bin = y[binary_filter]: creates the binary subset used for the binary logistic model\n",
    "\n",
    "Notes:\n",
    "- We binarize the data for a separate binary logistic regression example to illustrate ROC and AUC, which are straightforward for binary classification. For multiclass ROC-AUC we will need to binarize labels differently (see later cell).\n",
    "- Keeping imports explicit helps readability; unused imports (like pandas/seaborn) are harmless but can be removed if not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b6193",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Train binary logistic model\n",
    "model_bin = LogisticRegression()\n",
    "model_bin.fit(X_bin, y_bin)\n",
    "y_bin_pred = model_bin.predict(X_bin)\n",
    "y_bin_prob = model_bin.predict_proba(X_bin)[:, 1]\n",
    "\n",
    " # Evaluation metrics for binary classification\n",
    "print(\"=== Binary Logistic Regression ===\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_bin, y_bin_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_bin, y_bin_pred, target_names= target_names[:2]))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_bin, y_bin_prob):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b20fe2",
   "metadata": {},
   "source": [
    "### Explanation for binary logistic training and evaluation (Cell below trains the binary model)\n",
    "\n",
    "- model_bin = LogisticRegression(): creates a LogisticRegression object with default parameters. Common params: penalty (regularization), C (inverse regularization strength), solver (optimiser). Defaults are fine for small examples.\n",
    "- model_bin.fit(X_bin, y_bin): fits the logistic regression to the binary subset. Input shapes: X_bin (n_samples x n_features), y_bin (n_samples,)\n",
    "- y_bin_pred = model_bin.predict(X_bin): returns predicted class labels (0 or 1) for each sample\n",
    "- y_bin_prob = model_bin.predict_proba(X_bin)[:, 1]: returns predicted probabilities for the positive class (class 1). The output of predict_proba is shape (n_samples, n_classes); [:,1] selects column for class 1. For binary AUC we need a continuous score/probability, not just labels.\n",
    "\n",
    "Evaluation lines:\n",
    "- confusion_matrix(y_bin, y_bin_pred): compares true vs predicted labels to show counts of TP/FP/FN/TN\n",
    "- classification_report(...): prints precision, recall, f1-score and support for each class; target_names maps numeric labels to readable names\n",
    "- roc_auc_score(y_bin, y_bin_prob): computes AUC using true binary labels and predicted scores. AUC summarizes the ROC curve as a single number (1.0 perfect, 0.5 random).\n",
    "\n",
    "Notes and why [:,1] but not a single index for multiclass:\n",
    "- For binary classification predict_proba returns two columns (probabilities for class 0 and class 1). We select the positive-class column (class 1) as the score for ROC.\n",
    "- For multiclass, predict_proba returns probabilities for each class; when computing multiclass ROC-AUC we either compute AUC per class using that class's probability column or binarize labels and pass the full probability matrix to roc_auc_score with an averaging method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939735a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot ROC Curve (Binary)\n",
    "fpr, tpr, _ = roc_curve(y_bin, y_bin_prob)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (area = {auc(fpr, tpr):.2f})\", color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('Binary Logistic Regression - ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"binary_roc_curve.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2587a6",
   "metadata": {},
   "source": [
    "### Explanation for binary ROC plotting (Cell below creates the ROC plot)\n",
    "\n",
    "- fpr, tpr, _ = roc_curve(y_bin, y_bin_prob): computes the Receiver Operating Characteristic curve points.\n",
    "  - y_bin: true binary labels (0/1).\n",
    "  - y_bin_prob: predicted scores (probability of positive class).\n",
    "  - returns fpr (false positive rates), tpr (true positive rates), and thresholds (unused here, captured as _).\n",
    "  - These arrays are used to plot TPR vs FPR across decision thresholds.\n",
    "\n",
    "- plt.figure(figsize=(6,4)): creates a new figure with width 6in and height 4in.\n",
    "\n",
    "- plt.plot(fpr, tpr, label=f\"ROC Curve (area = {auc(fpr, tpr):.2f})\", color='blue'): plots the ROC curve.\n",
    "  - auc(fpr, tpr): computes area under curve from the two arrays.\n",
    "  - label uses an f-string to display the AUC value in the legend.\n",
    "  - color='blue' sets the curve color.\n",
    "\n",
    "- plt.plot([0, 1], [0, 1], 'k--'): plots a diagonal baseline line from (0,0) to (1,1).\n",
    "  - This diagonal represents a random classifier (TPR == FPR). If your ROC is above this line your model is better than random.\n",
    "  - 'k--' is a matplotlib shorthand where 'k' = black color and '--' = dashed line.\n",
    "\n",
    "- plt.title / plt.xlabel / plt.ylabel: set the plot title and axis labels.\n",
    "- plt.legend(): shows the legend containing the AUC label.\n",
    "- plt.grid(True): enables a grid to make it easier to read values.\n",
    "- plt.tight_layout(): adjusts spacing to prevent overlap of labels/legend.\n",
    "- plt.savefig('binary_roc_curve.png'): writes the figure to a PNG file.\n",
    "- plt.show(): displays the figure inline in the notebook.\n",
    "\n",
    "Notes and why ROC uses probabilities (scores) not labels:\n",
    "- ROC curves show tradeoffs across thresholds. If you only pass predicted labels you get a single (FPR,TPR) point, so using predicted probabilities (a continuous score) lets you sweep thresholds and form a curve.\n",
    "- auc(fpr,tpr) and roc_auc_score both summarize the ROC curve. roc_auc_score was used earlier for a numeric summary; auc(fpr,tpr) recomputes it from plotted arrays for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100980a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #------------------------\n",
    " # Multiclass Logistic Regression\n",
    " #------------------------\n",
    " # One-vs-Rest strategy (default in scikit-learn)\n",
    "model_multi = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "model_multi.fit(X, y)\n",
    "y_multi_pred = model_multi.predict(X)\n",
    "y_multi_prob = model_multi.predict_proba(X)\n",
    " # Binarize y for ROC-AUC calculation\n",
    "y_binarized = label_binarize(y, classes=[0, 1, 2])\n",
    " # Evaluation metrics for multiclass\n",
    "print(\"\\n=== Multiclass Logistic Regression ===\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_multi_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y, y_multi_pred, target_names=target_names))\n",
    "print(f\"Macro ROC-AUC Score: {roc_auc_score(y_binarized, y_multi_prob, average='macro'):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe879c2",
   "metadata": {},
   "source": [
    "### Explanation for multiclass training and ROC-AUC (Cell below trains multiclass model)\n",
    "\n",
    "- model_multi = LogisticRegression(multi_class='ovr', solver='liblinear'): creates a logistic model configured for multiclass using One-vs-Rest (OvR) strategy.\n",
    "  - multi_class='ovr': tells scikit-learn to train one binary classifier per class vs the rest. Alternative: 'multinomial' fits a single multinomial logistic regression.\n",
    "  - solver='liblinear': an optimizer suitable for small datasets and OvR; some solvers required for certain penalties or for multinomial.\n",
    "\n",
    "- model_multi.fit(X, y): fits the multiclass model on all three classes.\n",
    "- y_multi_pred = model_multi.predict(X): predicts integer class labels (0,1,2).\n",
    "- y_multi_prob = model_multi.predict_proba(X): returns an array shape (n_samples, n_classes) where each column is the predicted probability for that class.\n",
    "\n",
    "- y_binarized = label_binarize(y, classes=[0,1,2]): converts the integer labels into a binary indicator matrix with shape (n_samples, n_classes).\n",
    "  - Example: label 1 becomes [0,1,0]. This format is required to compute ROC curves/AUC per class because roc_curve expects binary labels for each class.\n",
    "\n",
    "- print(confusion_matrix(...)) and classification_report(...): same roles as in binary case but they’ll show multiclass metrics.\n",
    "- roc_auc_score(y_binarized, y_multi_prob, average='macro'): computes ROC-AUC by comparing the binarized true labels against the predicted probability matrix.\n",
    "  - average='macro' computes AUC for each class and then averages them equally (unweighted by class support). Other options: 'weighted' (weighted by support), 'micro' (aggregate contributions of all classes).\n",
    "\n",
    "Why we binarize y for multiclass ROC-AUC:\n",
    "- ROC/AUC is defined for binary labels (true positive vs false positive for a given class). To extend to multiclass we compute one-vs-rest curves per class: binarize the true labels so each class becomes a separate binary problem.\n",
    "- After binarization, we either compute per-class AUCs and average, or use methods that aggregate predictions.\n",
    "\n",
    "Why there’s no [:,1] in the multiclass probability use here:\n",
    "- In binary example we used [:,1] to select the positive-class probability for ROC. In multiclass we keep the full probability matrix (n_samples x n_classes) because roc_auc_score can accept the full score matrix together with a binarized y to compute per-class AUCs in one call.\n",
    "\n",
    "Notes and practical tips:\n",
    "- For small datasets scikit-learn defaults usually work; for larger/more complex problems you’ll tune 'C', choose different solvers, or use class_weight to handle imbalanced classes.\n",
    "- When using 'ovr' you can still compute per-class ROC from y_multi_prob[:, i] vs y_binarized[:, i] if you prefer to plot them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot ROC curves for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_binarized[:, i], y_multi_prob[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.figure(figsize=(6,4))\n",
    "for i, color in zip(range(3), ['blue', 'green', 'red']):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "              label=f\"Class {target_names[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('Multiclass Logistic Regression - ROC Curves')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"multiclass_roc_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a68c6",
   "metadata": {},
   "source": [
    "### Explanation for multiclass ROC plotting (last plotting cell)\n",
    "\n",
    "- fpr = {}, tpr = {}, roc_auc = {}: initialize dictionaries to store arrays and AUC values per class. Using dicts lets us index by class i (0,1,2).\n",
    "\n",
    "- for i in range(3):\n",
    "    - fpr[i], tpr[i], _ = roc_curve(y_binarized[:, i], y_multi_prob[:, i]): computes ROC curve for class i vs rest.\n",
    "      - y_binarized[:, i] is the binary true-label vector for class i (1 where true class==i, 0 otherwise).\n",
    "      - y_multi_prob[:, i] is the predicted probability for class i from predict_proba. We pass these two to get per-class ROC points.\n",
    "    - roc_auc[i] = auc(fpr[i], tpr[i]): numeric area under the per-class ROC curve.\n",
    "    - plt.figure(figsize=(6,4)): create a new figure for each class (so each class can be plotted on its own or combined depending on plotting approach).\n",
    "\n",
    "- for i, color in zip(range(3), ['blue','green','red']):\n",
    "    - plt.plot(fpr[i], tpr[i], color=color, label=...): draws the ROC curve for class i and adds the AUC value in the legend.\n",
    "    - plt.plot([0,1],[0,1], 'k--'): draws the baseline diagonal indicating random classifier performance. The further above the curve is compared to this diagonal, the better the classifier.\n",
    "\n",
    "- plt.title / plt.xlabel / plt.ylabel / plt.legend / plt.grid / plt.tight_layout / plt.savefig / plt.show: same roles as earlier (labels, legend, grid, layout, save and show the combined figure).\n",
    "\n",
    "Why we compute per-class ROC and use dicts/arrays:\n",
    "- ROC is inherently binary; for multiclass we compute one-vs-rest per class, so we need separate (fpr,tpr) arrays per class. Storing them in dicts by class index keeps code organized.\n",
    "- y_multi_prob is shape (n_samples, n_classes); indexing [:, i] picks the score column for class i. This mirrors the binary case where [:,1] picked the positive-class probability.\n",
    "\n",
    "Plotting choices and interpretation tips:\n",
    "- You can either plot each class on the same figure (so curves overlap and are colored differently) or create separate figures per class; this code sets up per-class curves and plots them together for comparison.\n",
    "- Use average='macro' or 'weighted' depending on whether you want equal weighting per class or weighting by class support when reporting a single scalar AUC.\n",
    "- If classes are imbalanced prefer weighted averages or per-class reporting to see which classes the model struggles with.\n",
    "\n",
    "Practical note on small dataset behavior:\n",
    "- With only 50 samples per class (Iris dataset), ROC curves can look optimistic; always validate with cross-validation or a separate test set.\n",
    "\n",
    "Summary:\n",
    "- The plotting cell computes and visualizes per-class ROC curves using binarized true labels and predicted per-class probabilities. The diagonal baseline 'k--' shows random performance; AUC summarises curve quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f735b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
